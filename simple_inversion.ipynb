{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inversion of 2 by 2 matrices using an operator recurrent neural network\n",
    "\n",
    "We use a simplified version of the network architecture proposed in the preprint\n",
    "\n",
    "> Maarten V. de Hoop, Matti Lassas, Christopher A. Wong. _Deep learning architectures for nonlinear operator functions and nonlinear inverse problems_. [arXiv:1912.11090](https://arxiv.org/abs/1912.11090)\n",
    "\n",
    "and teach it to invert matrices $X$ of the form $X = R D R^T$ where\n",
    "\n",
    "$$\n",
    "R = \\begin{pmatrix}\n",
    "c & -s\n",
    "\\\\\n",
    "s & c\n",
    "\\end{pmatrix},\n",
    "\\quad\n",
    "D = \\begin{pmatrix}\n",
    "\\lambda_1 & 0\n",
    "\\\\\n",
    "0 & \\lambda_2\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "$c = \\cos(\\alpha)$ and $s = \\sin(\\alpha)$ for some $\\alpha \\in (0,2\\pi)$,\n",
    "and $\\lambda_j \\in (1/2, 3/2)$, $j=1,2$.\n",
    "\n",
    "We use notations as in version 3 of the preprint (revised 3 Jan 2022). The notation is different in earlier version.\n",
    "\n",
    "In the code, variables have the same meaning as in the [Quickstart](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html) guige of PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "The operator recurrent architecture is implemented in `opnet` module, and \n",
    "generation of learning data in `simple_inversion_data`. \n",
    "\n",
    "File `PATH` is used to save the parameters of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import opnet\n",
    "from simple_inversion_data import generate_data, save_data, load_data\n",
    "\n",
    "PATH = './simple_inversion_net4.pth'\n",
    "# PATH = './simple_inversion_netPlusMinusReLU.pth'\n",
    "# PATH = './simple_inversion_netNEGA.pth'\n",
    "# PATH = './simple_inversion_netReLU.pth'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the network model and the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dim = 2 # use 2 x 2 matrices\n",
    "num_layers = 5\n",
    "# luodaan uusi neuroverkko\n",
    "#model = opnet.OperatorNet(dim, 2*num_layers, useReLU=False)\n",
    "model = opnet.OperatorNet(dim, num_layers, useReLU=True) \n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of training data\n",
    "\n",
    "Training data consists of pairs $(X,y)$ where $X$ is an invertible $2 \\times 2$ matrix and $y = X^{-1} v$\n",
    "where $v = (1,1) \\in \\mathbb{R}^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_data(*generate_data(60000), \"simple_inversion_train_dataPOSNEG.npz\")\n",
    "#save_data(*generate_data(10000), \"simple_inversion_test_dataPOSNEG.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr=  0.1\n",
      "kierros  1\n",
      "True: \n",
      "tensor([[[0.8340],\n",
      "         [0.7229]],\n",
      "\n",
      "        [[0.7651],\n",
      "         [0.7791]]])\n",
      "Prediction: \n",
      "tensor([[[0.8409],\n",
      "         [0.7290]],\n",
      "\n",
      "        [[0.7676],\n",
      "         [0.7874]]])\n",
      "Avg loss: 0.000044\n",
      "kierros  2\n",
      "True: \n",
      "tensor([[[0.8340],\n",
      "         [0.7229]],\n",
      "\n",
      "        [[0.7651],\n",
      "         [0.7791]]])\n",
      "Prediction: \n",
      "tensor([[[0.8409],\n",
      "         [0.7290]],\n",
      "\n",
      "        [[0.7676],\n",
      "         [0.7873]]])\n",
      "Avg loss: 0.000043\n",
      "Parameters after ReLU:\n",
      "Layer:\n",
      "OperatorLayer()\n",
      "{'training': True, '_parameters': OrderedDict([('A', Parameter containing:\n",
      "tensor([[ 0.4311,  0.1477],\n",
      "        [-0.5680,  0.2380]], requires_grad=True)), ('B', Parameter containing:\n",
      "tensor([[-0.1660,  0.0232],\n",
      "        [-0.0747,  0.0750]], requires_grad=True)), ('b', Parameter containing:\n",
      "tensor([[ 0.5435],\n",
      "        [-0.1295]], requires_grad=True))]), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict(), 'dim': 2}\n",
      "Layer:\n",
      "OperatorLayer()\n",
      "{'training': True, '_parameters': OrderedDict([('A', Parameter containing:\n",
      "tensor([[-0.2993, -0.2902],\n",
      "        [ 0.2829, -0.4803]], requires_grad=True)), ('B', Parameter containing:\n",
      "tensor([[-0.1922, -0.3022],\n",
      "        [ 0.4845, -0.0724]], requires_grad=True)), ('b', Parameter containing:\n",
      "tensor([[-0.0789],\n",
      "        [ 0.3710]], requires_grad=True))]), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict(), 'dim': 2}\n",
      "Layer:\n",
      "OperatorLayer()\n",
      "{'training': True, '_parameters': OrderedDict([('A', Parameter containing:\n",
      "tensor([[ 0.2804,  0.3516],\n",
      "        [-0.2189,  0.4144]], requires_grad=True)), ('B', Parameter containing:\n",
      "tensor([[-0.4995,  0.1090],\n",
      "        [ 0.2665, -0.6373]], requires_grad=True)), ('b', Parameter containing:\n",
      "tensor([[ 0.3849],\n",
      "        [-0.2231]], requires_grad=True))]), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict(), 'dim': 2}\n",
      "Layer:\n",
      "OperatorLayer()\n",
      "{'training': True, '_parameters': OrderedDict([('A', Parameter containing:\n",
      "tensor([[-0.6260, -0.7154],\n",
      "        [ 0.0919, -0.5384]], requires_grad=True)), ('B', Parameter containing:\n",
      "tensor([[ 0.6018,  0.0853],\n",
      "        [-0.2944,  0.2584]], requires_grad=True)), ('b', Parameter containing:\n",
      "tensor([[-0.6820],\n",
      "        [-1.0638]], requires_grad=True))]), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict(), 'dim': 2}\n",
      "Layer:\n",
      "OperatorLayer()\n",
      "{'training': True, '_parameters': OrderedDict([('A', Parameter containing:\n",
      "tensor([[-0.8008,  0.4328],\n",
      "        [ 0.1163, -0.7739]], requires_grad=True)), ('B', Parameter containing:\n",
      "tensor([[ 0.8311, -0.3985],\n",
      "        [ 0.0341,  0.7300]], requires_grad=True)), ('b', Parameter containing:\n",
      "tensor([[0.2838],\n",
      "        [0.0091]], requires_grad=True))]), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict(), 'dim': 2}\n",
      "Layer:\n",
      "OperatorLayer()\n",
      "{'training': True, '_parameters': OrderedDict([('A', Parameter containing:\n",
      "tensor([[0.0336, 0.4868],\n",
      "        [0.2199, 0.0287]], requires_grad=True)), ('B', Parameter containing:\n",
      "tensor([[-0.5608,  0.5262],\n",
      "        [ 0.4997,  0.3438]], requires_grad=True)), ('b', Parameter containing:\n",
      "tensor([[-0.6879],\n",
      "        [-0.3291]], requires_grad=True))]), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict(), 'dim': 2}\n",
      "Layer:\n",
      "OperatorLayer()\n",
      "{'training': True, '_parameters': OrderedDict([('A', Parameter containing:\n",
      "tensor([[ 0.4586, -0.5669],\n",
      "        [ 0.4929,  0.5866]], requires_grad=True)), ('B', Parameter containing:\n",
      "tensor([[-0.5114, -0.3641],\n",
      "        [-0.5852,  0.6454]], requires_grad=True)), ('b', Parameter containing:\n",
      "tensor([[1.1926],\n",
      "        [0.0618]], requires_grad=True))]), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict(), 'dim': 2}\n",
      "Layer:\n",
      "OperatorLayer()\n",
      "{'training': True, '_parameters': OrderedDict([('A', Parameter containing:\n",
      "tensor([[0.5420, 0.3412],\n",
      "        [0.3834, 0.3961]], requires_grad=True)), ('B', Parameter containing:\n",
      "tensor([[-8.2266e-01, -3.2372e-04],\n",
      "        [-1.1381e-01, -2.5640e-01]], requires_grad=True)), ('b', Parameter containing:\n",
      "tensor([[0.2212],\n",
      "        [0.8206]], requires_grad=True))]), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict(), 'dim': 2}\n",
      "Layer:\n",
      "OperatorLayer()\n",
      "{'training': True, '_parameters': OrderedDict([('A', Parameter containing:\n",
      "tensor([[-0.4038,  0.7122],\n",
      "        [-0.1226, -0.5192]], requires_grad=True)), ('B', Parameter containing:\n",
      "tensor([[0.5652, 0.0185],\n",
      "        [0.2000, 1.1920]], requires_grad=True)), ('b', Parameter containing:\n",
      "tensor([[-0.2219],\n",
      "        [ 0.1316]], requires_grad=True))]), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict(), 'dim': 2}\n",
      "Layer:\n",
      "OperatorLayer()\n",
      "{'training': True, '_parameters': OrderedDict([('A', Parameter containing:\n",
      "tensor([[-0.5625, -0.4310],\n",
      "        [-0.1102, -0.5896]], requires_grad=True)), ('B', Parameter containing:\n",
      "tensor([[0.5319, 0.3565],\n",
      "        [0.0150, 0.4868]], requires_grad=True)), ('b', Parameter containing:\n",
      "tensor([[0.6859],\n",
      "        [0.9206]], requires_grad=True))]), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict(), 'dim': 2}\n",
      "Layer:\n",
      "ReLU()\n",
      "{'training': True, '_parameters': OrderedDict(), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict(), 'inplace': False}\n",
      "Layer:\n",
      "ReLU()\n",
      "{'training': True, '_parameters': OrderedDict(), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict(), 'inplace': False}\n",
      "Layer:\n",
      "ReLU()\n",
      "{'training': True, '_parameters': OrderedDict(), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict(), 'inplace': False}\n",
      "Layer:\n",
      "ReLU()\n",
      "{'training': True, '_parameters': OrderedDict(), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict(), 'inplace': False}\n",
      "Layer:\n",
      "ReLU()\n",
      "{'training': True, '_parameters': OrderedDict(), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict(), 'inplace': False}\n",
      "testi on  Parameter containing:\n",
      "tensor([[ 0.4311,  0.1477],\n",
      "        [-0.5680,  0.2380]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import training_and_testing\n",
    "import os.path\n",
    "\n",
    "# update changes\n",
    "from importlib import reload \n",
    "reload(training_and_testing)\n",
    "reload(opnet)\n",
    "\n",
    "lr=1e-1\n",
    "# upload the neural network used earlier:\n",
    "if os.path.exists(PATH):\n",
    "    #print(\"vittu jeejee\")\n",
    "    model.load_state_dict(torch.load(PATH)) \n",
    "else:\n",
    "    print(\"no old PATH, I'll make a new one\")\n",
    "\n",
    "training_and_testing.training_and_testing(model, loss_fn, lr)\n",
    "\n",
    "#chech if network uses relu\n",
    "doesit = model.does_it_use_relu()\n",
    "#layer = model.layers[0].A\n",
    "#print(\"testi layer on \", layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     load_data(\"simple_inversion_train_data.npz\"), \n",
    "#     batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the optimization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate parameter is from the quickstart guide \n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over the training data multiple times (epochs) and \n",
    "save the optimized parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(2): \n",
    "#     print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "#     for batch, (X, y) in enumerate(train_loader):\n",
    "#         # Compute prediction error\n",
    "#         pred = model(X)\n",
    "#         loss = loss_fn(pred, y)\n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         # Print statistics\n",
    "#         if batch % 100 == 0:\n",
    "#             n, N = (batch + 1) * len(X), len(train_loader.dataset)\n",
    "#             print(f\"loss: {loss.item():>7f}  [{n:>5d}/{N:>5d}]\")\n",
    "\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "If we have already trained the network, we can just load its parameters. (Note that we still need to run the initialization.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load trained variables\n",
    "# model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the testing data\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     load_data(\"simple_inversion_test_data.npz\"), \n",
    "#     batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute a couple of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataiter = iter(test_loader)\n",
    "# X, y = dataiter.next()\n",
    "# with torch.no_grad():\n",
    "#     pred = model(X)\n",
    "# print(\"True: \")\n",
    "# print(y[:2])\n",
    "# print(\"Prediction: \")\n",
    "# print(pred[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_batches = len(test_loader)\n",
    "# test_loss = 0\n",
    "# with torch.no_grad():\n",
    "#     for X, y in test_loader:\n",
    "#         pred = model(X)\n",
    "#         test_loss += loss_fn(pred, y).item()\n",
    "# test_loss /= num_batches\n",
    "# print(f\"Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f2046300ef7c0db0fa8abb336d3efc92823433443e4058b7e3e3125d8bae97b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
